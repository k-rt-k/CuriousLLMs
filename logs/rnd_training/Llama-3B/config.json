{
  "learning_rate": 7e-05,
  "dataset_builder": {
    "batch_size": 128,
    "model_name_for_tokenizer": "meta-llama/Llama-3.2-3B",
    "renderer_name": "role_colon",
    "group_size": 16,
    "convo_prefix": "standard",
    "seed": 0,
    "math_train_size": 6000,
    "deepmath_train_size": 6000,
    "deepmath_test_size": 500,
    "deepmath_seed": 42
  },
  "model_name": "meta-llama/Llama-3.2-3B",
  "max_tokens": 2048,
  "compute_post_kl": false,
  "evaluator_builders": [],
  "lora_rank": 32,
  "kl_penalty_coef": 0.0,
  "kl_discount_factor": 0.0,
  "loss_fn": "ppo",
  "num_substeps": 1,
  "wandb_project": "rnd_train",
  "wandb_name": "Llama-3B",
  "log_path": "logs/rnd_training/Llama-3B",
  "base_url": null,
  "enable_trace": false,
  "remove_constant_reward_groups": false,
  "eval_every": 20,
  "save_every": 20,
  "load_checkpoint_path": null,
  "async_config": null,
  "stream_minibatch_config": null,
  "num_groups_to_log": 4,
  "use_reasoning_rewards": true,
  "reasoning_reward_coef": 0.5,
  "use_rnd_curiosity": true,
  "rnd_buffer_size_multiplier": 3,
  "rnd_update_steps": 50,
  "rnd_minibatch_size": 1024,
  "curiosity_reward_coef": 0.2,
  "rnd_learning_rate": 0.001,
  "group_size": 16,
  "penalize_incorrect_novelty": true,
  "correctness_threshold": 0.6,
  "curiosity_warmup_batches": 20
}