2025-12-01 19:46:48,146 INFO    MainThread:1797622 [wandb_setup.py:_flush():81] Current SDK version is 0.22.3
2025-12-01 19:46:48,146 INFO    MainThread:1797622 [wandb_setup.py:_flush():81] Configure stats pid to 1797622
2025-12-01 19:46:48,146 INFO    MainThread:1797622 [wandb_setup.py:_flush():81] Loading settings from /home/hpoonia/.config/wandb/settings
2025-12-01 19:46:48,146 INFO    MainThread:1797622 [wandb_setup.py:_flush():81] Loading settings from /home/hpoonia/CuriousLLMs/wandb/settings
2025-12-01 19:46:48,146 INFO    MainThread:1797622 [wandb_setup.py:_flush():81] Loading settings from environment variables
2025-12-01 19:46:48,146 INFO    MainThread:1797622 [wandb_init.py:setup_run_log_directory():706] Logging user logs to logs/rnd_training/Llama-3B/wandb/run-20251201_194648-h7i22pap/logs/debug.log
2025-12-01 19:46:48,146 INFO    MainThread:1797622 [wandb_init.py:setup_run_log_directory():707] Logging internal logs to logs/rnd_training/Llama-3B/wandb/run-20251201_194648-h7i22pap/logs/debug-internal.log
2025-12-01 19:46:48,146 INFO    MainThread:1797622 [wandb_init.py:init():833] calling init triggers
2025-12-01 19:46:48,146 INFO    MainThread:1797622 [wandb_init.py:init():838] wandb.init called with sweep_config: {}
config: {'learning_rate': 7e-05, 'dataset_builder': {'batch_size': 128, 'model_name_for_tokenizer': 'meta-llama/Llama-3.2-3B', 'renderer_name': 'role_colon', 'group_size': 16, 'convo_prefix': 'standard', 'seed': 0, 'math_train_size': 6000, 'deepmath_train_size': 6000, 'deepmath_test_size': 500, 'deepmath_seed': 42}, 'model_name': 'meta-llama/Llama-3.2-3B', 'max_tokens': 2048, 'compute_post_kl': False, 'evaluator_builders': [], 'lora_rank': 32, 'kl_penalty_coef': 0.0, 'kl_discount_factor': 0.0, 'loss_fn': 'ppo', 'num_substeps': 1, 'wandb_project': 'rnd_train', 'wandb_name': 'Llama-3B', 'log_path': 'logs/rnd_training/Llama-3B', 'base_url': None, 'enable_trace': False, 'remove_constant_reward_groups': False, 'eval_every': 20, 'save_every': 20, 'load_checkpoint_path': None, 'async_config': None, 'stream_minibatch_config': None, 'num_groups_to_log': 4, 'use_reasoning_rewards': True, 'reasoning_reward_coef': 0.5, 'use_rnd_curiosity': True, 'rnd_buffer_size_multiplier': 3, 'rnd_update_steps': 50, 'rnd_minibatch_size': 1024, 'curiosity_reward_coef': 0.2, 'rnd_learning_rate': 0.001, 'group_size': 16, 'penalize_incorrect_novelty': True, 'correctness_threshold': 0.6, 'curiosity_warmup_batches': 20, '_wandb': {}}
2025-12-01 19:46:48,147 INFO    MainThread:1797622 [wandb_init.py:init():881] starting backend
2025-12-01 19:46:48,356 INFO    MainThread:1797622 [wandb_init.py:init():884] sending inform_init request
2025-12-01 19:46:48,361 INFO    MainThread:1797622 [wandb_init.py:init():892] backend started and connected
2025-12-01 19:46:48,363 INFO    MainThread:1797622 [wandb_init.py:init():962] updated telemetry
2025-12-01 19:46:48,372 INFO    MainThread:1797622 [wandb_init.py:init():986] communicating run to backend with 90.0 second timeout
2025-12-01 19:46:48,792 INFO    MainThread:1797622 [wandb_init.py:init():1033] starting run threads in backend
2025-12-01 19:46:49,163 INFO    MainThread:1797622 [wandb_run.py:_console_start():2506] atexit reg
2025-12-01 19:46:49,163 INFO    MainThread:1797622 [wandb_run.py:_redirect():2354] redirect: wrap_raw
2025-12-01 19:46:49,163 INFO    MainThread:1797622 [wandb_run.py:_redirect():2423] Wrapping output streams.
2025-12-01 19:46:49,164 INFO    MainThread:1797622 [wandb_run.py:_redirect():2446] Redirects installed.
2025-12-01 19:46:49,166 INFO    MainThread:1797622 [wandb_init.py:init():1073] run started, returning control to user process
2025-12-01 19:46:49,197 INFO    MainThread:1797622 [wandb_run.py:_config_callback():1390] config_cb None None {'learning_rate': 7e-05, 'dataset_builder': {'batch_size': 128, 'model_name_for_tokenizer': 'meta-llama/Llama-3.2-3B', 'renderer_name': 'role_colon', 'group_size': 16, 'convo_prefix': 'standard', 'seed': 0, 'math_train_size': 6000, 'deepmath_train_size': 6000, 'deepmath_test_size': 500, 'deepmath_seed': 42}, 'model_name': 'meta-llama/Llama-3.2-3B', 'max_tokens': 2048, 'compute_post_kl': False, 'evaluator_builders': [], 'lora_rank': 32, 'kl_penalty_coef': 0.0, 'kl_discount_factor': 0.0, 'loss_fn': 'ppo', 'num_substeps': 1, 'wandb_project': 'rnd_train', 'wandb_name': 'Llama-3B', 'log_path': 'logs/rnd_training/Llama-3B', 'base_url': None, 'enable_trace': False, 'remove_constant_reward_groups': False, 'eval_every': 20, 'save_every': 20, 'load_checkpoint_path': None, 'async_config': None, 'stream_minibatch_config': None, 'num_groups_to_log': 4, 'use_reasoning_rewards': True, 'reasoning_reward_coef': 0.5, 'use_rnd_curiosity': True, 'rnd_buffer_size_multiplier': 3, 'rnd_update_steps': 50, 'rnd_minibatch_size': 1024, 'curiosity_reward_coef': 0.2, 'rnd_learning_rate': 0.001, 'group_size': 16, 'penalize_incorrect_novelty': True, 'correctness_threshold': 0.6, 'curiosity_warmup_batches': 20}
